{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45628d30-bdfd-4ee0-9b7c-9ff92ec32cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 1 done\n"
     ]
    }
   ],
   "source": [
    "# ollama serve\n",
    "# ollama pull nomic-embed-text\n",
    "\n",
    "from pymilvus import MilvusClient, FieldSchema, DataType, Collection, connections\n",
    "import ollama\n",
    "\n",
    "conversation = [\n",
    "    \"Alice: Hey, how was your weekend?\",\n",
    "    \"Bob: Pretty good! I went hiking. You?\",\n",
    "    \"Alice: Nice! I just relaxed and watched a few movies.\",\n",
    "    \"Bob: That sounds great. Anything worth recommending?\",\n",
    "    \"Alice: Yeah, 'The Secret Garden' was surprisingly good!\",\n",
    "    \"Bob: I'll check it out. Thanks!\"\n",
    "]\n",
    "\n",
    "records_dict = []\n",
    "for i, line in enumerate(conversation):\n",
    "    records_dict.append(\n",
    "        {\n",
    "            \"id\": i+1,\n",
    "            \"original_text\": line,\n",
    "            \"vector\": ollama.embeddings(model='nomic-embed-text', prompt=line)['embedding']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "# print(records_dict)\n",
    "print(\"part 1 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43edf740-a4e7-49db-a4e3-3be6262f1a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymilvus.milvus_client.milvus_client.MilvusClient object at 0x117ed6ba0>\n",
      "exists:True\n",
      "collection: <Collection>:\n",
      "-------------\n",
      "<name>: test_textvector_storage\n",
      "<description>: \n",
      "<schema>: {'auto_id': False, 'description': '', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'original_text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 512}}, {'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'enable_dynamic_field': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a milvus collection\n",
    "client = MilvusClient(\n",
    "    uri=\"http://localhost:19530\",\n",
    "    token=\"root:Milvus\"\n",
    ")\n",
    "print(client)\n",
    "\n",
    "collection_name = \"test_textvector_storage\"\n",
    "exists = client.has_collection(collection_name=collection_name)\n",
    "print(f\"exists:{exists}\")\n",
    "\n",
    "def create_collection(client):\n",
    "    # 3.1. Create schema\n",
    "    schema = MilvusClient.create_schema(\n",
    "        enable_dynamic_field=True,\n",
    "    )\n",
    "    \n",
    "    # 3.2. Add fields to schema\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"original_text\", datatype=DataType.VARCHAR, max_length=512)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=768)\n",
    "    \n",
    "    # 3.3. Prepare index parameters\n",
    "    index_params = client.prepare_index_params()\n",
    "    \n",
    "    # 3.4. Add indexes\n",
    "    index_params.add_index(\n",
    "        field_name=\"id\",\n",
    "        index_type=\"AUTOINDEX\"\n",
    "    )\n",
    "    \n",
    "    index_params.add_index(\n",
    "        field_name=\"vector\", \n",
    "        index_type=\"AUTOINDEX\",\n",
    "        metric_type=\"COSINE\"\n",
    "    )\n",
    "    \n",
    "    return client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        index_params=index_params\n",
    "    )\n",
    "        \n",
    "if not exists:\n",
    "    print(create_collection(client))\n",
    "\n",
    "connections.connect(alias=\"default\", host=\"127.0.0.1\", port=\"19530\")\n",
    "\n",
    "collection = Collection(collection_name)\n",
    "collection.load()\n",
    "print(f\"collection: {collection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eccd6a0-3ae3-4922-85f2-873961ed9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'insert_count': 6, 'ids': [1, 2, 3, 4, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "# now to insert\n",
    "res = client.insert(\n",
    "    collection_name=collection_name,\n",
    "    data=records_dict\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775712cd-55ab-403a-acb8-ccd7039bb0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: Hey, how was your weekend?;Alice: Nice! I just relaxed and watched a few movies.;Bob: Pretty good! I went hiking. You?;\n"
     ]
    }
   ],
   "source": [
    "# now retrieve the documents based on prompt\n",
    "# question = \"what did everyone do in their weekend?\"\n",
    "question = \"Who did what in weekend\"\n",
    "prompt_vector = ollama.embeddings(model='nomic-embed-text', prompt=question)['embedding']\n",
    "\n",
    "res = collection.search(\n",
    "    anns_field=\"vector\",\n",
    "    data=[prompt_vector],\n",
    "    param={\"nprobe\": 16},\n",
    "    limit=3,\n",
    "    output_fields=[\"original_text\"]\n",
    ")\n",
    "\n",
    "\n",
    "# fusing the gathered information\n",
    "retrieved_documents = \"\"\n",
    "milvus_results_array = []\n",
    "for hits in res:\n",
    "    for hit in hits:\n",
    "        milvus_results_array.append(hit)\n",
    "        retrieved_documents = retrieved_documents + hit['entity']['original_text'] + \";\"\n",
    "\n",
    "print(retrieved_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b584abd6-81e9-46b8-8eb5-2975dc981d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer this question: Who did what in weekend Based on the information here: Alice: Hey, how was your weekend?;Alice: Nice! I just relaxed and watched a few movies.;Bob: Pretty good! I went hiking. You?;\n",
      "-----------------\n",
      "Based on the information provided, Alice relaxed and watched movies, while Bob went hiking.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='gemma:7b', messages=[{\n",
    "    'role': 'user', \n",
    "    'content': f\"Answer this question: {question} Based on the information here: {retrieved_documents}\",\n",
    "}])\n",
    "print(f\"Answer this question: {question} Based on the information here: {retrieved_documents}\")\n",
    "print(\"-----------------\")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee18d0a5-265f-4786-8455-66e251b09de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "    return output\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    inputs = tokenizer(\n",
    "        pairs, padding=False, truncation='longest_first',\n",
    "        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    )\n",
    "    for i, ele in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logits(inputs, **kwargs):\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "    return scores\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_length = 8192\n",
    "\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. \n",
    "                Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e680d415-82ec-48ae-82fb-d9e9aa7df4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'distance': 0.6612014770507812, 'entity': {'original_text': 'Alice: Hey, how was your weekend?'}}, {'id': 3, 'distance': 0.42717790603637695, 'entity': {'original_text': 'Alice: Nice! I just relaxed and watched a few movies.'}}, {'id': 2, 'distance': 0.4147673547267914, 'entity': {'original_text': 'Bob: Pretty good! I went hiking. You?'}}]\n",
      "['Alice: Hey, how was your weekend?', 'Alice: Nice! I just relaxed and watched a few movies.', 'Bob: Pretty good! I went hiking. You?']\n",
      "scores:  [0.08847928047180176, 0.06078891456127167, 0.03812427073717117]\n"
     ]
    }
   ],
   "source": [
    "task = 'Retrieve passages where someone said they did something'\n",
    "\n",
    "queries = [question]\n",
    "\n",
    "# print(res)\n",
    "# print([hit for hits in res])\n",
    "\n",
    "print(milvus_results_array)\n",
    "\n",
    "documents = [r['entity']['original_text'] for r in milvus_results_array]\n",
    "print(documents)\n",
    "\n",
    "pairs = [format_instruction(task, queries[0], doc) for doc in (documents)]\n",
    "\n",
    "# # Tokenize the input texts\n",
    "inputs = process_inputs(pairs)\n",
    "scores = compute_logits(inputs)\n",
    "\n",
    "print(\"scores: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb7ff7-b66b-44f8-8106-52b21f91c49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
